{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Cài đặt\n",
        "!pip install ollama fastapi uvicorn pydantic nest-asyncio pyngrok colab-xterm\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ],
      "metadata": {
        "id": "i7ts7ERicpQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Khởi chạy mọi thứ (Ollama + Tải model + FastAPI)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import ollama\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# --- 1. Khởi chạy \"Bộ não\" OLLAMA (Cổng 11434) ---\n",
        "env = os.environ.copy()\n",
        "env[\"OLLAMA_HOST\"] = \"0.0.0.0\"\n",
        "env[\"OLLAMA_ORIGINS\"] = \"*\"\n",
        "\n",
        "def run_ollama_serve():\n",
        "    print(\"Starting Ollama server...\")\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], env=env, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    print(\"Ollama server process started.\")\n",
        "\n",
        "ollama_thread = threading.Thread(target=run_ollama_serve)\n",
        "ollama_thread.start()\n",
        "time.sleep(5) # Đợi Ollama khởi động\n",
        "\n",
        "# --- 2. TẢI MÔ HÌNH  ---\n",
        "try:\n",
        "    print(\"Pulling model 'llama3'. This may take a few minutes...\")\n",
        "    # Lệnh này sẽ chạy và chờ cho đến khi tải xong\n",
        "    subprocess.run([\"ollama\", \"pull\", \"llama3\"], check=True)\n",
        "    print(\"Model 'llama3' pulled successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error pulling model: {e}\")\n",
        "    # Nếu lỗi ở đây, bạn cần kiểm tra lại\n",
        "\n",
        "# --- 3. Định nghĩa \"Giao diện API\" FASTAPI (Cổng 8000) ---\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "# Khởi tạo client Ollama\n",
        "ollama_client = ollama.Client(host='http://localhost:11434')\n",
        "\n",
        "# Cấu trúc data mà Streamlit sẽ gửi\n",
        "class ItineraryRequest(BaseModel):\n",
        "    origin: str\n",
        "    destination: str\n",
        "    duration_days: int\n",
        "    interests: str\n",
        "    pace: str\n",
        "\n",
        "@app.post(\"/generate_itinerary\")\n",
        "async def generate_itinerary(request: ItineraryRequest):\n",
        "    print(f\"Received request for {request.destination}\")\n",
        "\n",
        "    # Prompt (Lời nhắc) chi tiết\n",
        "    prompt = f\"\"\"\n",
        "    Bạn là một chuyên gia lập kế hoạch du lịch. Hãy tạo một lịch trình du lịch chi tiết cho chuyến đi {request.duration_days} ngày đến {request.destination}.\n",
        "    Thông tin chuyến đi:\n",
        "    - Khởi hành từ: {request.origin}\n",
        "    - Sở thích: {request.interests}\n",
        "    - Tốc độ chuyến đi: {request.pace}\n",
        "    Yêu cầu đầu ra:\n",
        "    - Chỉ trả về nội dung lịch trình.\n",
        "    - Định dạng nghiêm ngặt bằng Markdown.\n",
        "    - Phân chia theo từng ngày (ví dụ: ## Ngày 1: Tên chủ đề).\n",
        "    - Mỗi ngày phải có ba phần: ### Buổi sáng, ### Buổi chiều, và ### Buổi tối.\n",
        "    - Cung cấp giải thích ngắn gọn (1-2 câu) cho mỗi hoạt động.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = ollama_client.chat(\n",
        "            model='llama3', # Dùng mô hình đã tải\n",
        "            messages=[{'role': 'user', 'content': prompt}]\n",
        "        )\n",
        "        itinerary_text = response['message']['content']\n",
        "        return {\"itinerary\": itinerary_text}\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Ollama: {e}\")\n",
        "        return {\"error\": \"Failed to generate itinerary from Ollama.\"}\n",
        "\n",
        "# --- 4. Chạy máy chủ FastAPI (Cổng 8000) ---\n",
        "def run_fastapi_server():\n",
        "    print(\"Starting FastAPI server on port 8000...\")\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# nest_asyncio cho phép uvicorn chạy trong môi trường notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "fastapi_thread = threading.Thread(target=run_fastapi_server)\n",
        "fastapi_thread.start()\n",
        "print(\"Backend setup complete. Both Ollama and FastAPI are running.\")"
      ],
      "metadata": {
        "id": "taKBfxEYcsph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khi thanh terminal hiện ra nhấn\n",
        "ssh -p 443 -R0:localhost:8000 qr@a.pinggy.io\n",
        "để tiếp tục\n"
      ],
      "metadata": {
        "id": "FPsUSZ-IeIdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Khởi động Terminal\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "hpw6v4ePc25f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}